
# Chapter 2. 넘파이로 공부하는 선형대수

## 2.1 데이터와 행렬

### 데이터의 유형

선형 대수에서 다루는 데이터의 종류

- 스칼라 : 숫자 하나로 이루어진 데이터
- 벡터 : 여러 숫자로 이루어진 데이터 레코드
- 행렬 : 벡터, 즉 데이터 레고크다 여럿인 데이터 집합
- 텐서 : 같은 크기의 행렬이 여러개 있는 것


## 2.2 벡터와 행렬의 연산

### 벡터/행렬의 덧셈과 뺄셈 

=> 요소별 연산

### 스칼라와 벡터/행렬의 곱셈

벡터 $x$ 또는 행렬 $A$의 모든 원소에 스칼라 값 $c$를 곱하는 것

### 브로드캐스팅

원래 덧셈과 뺄셈은 크기(차원)가 같은 두 벡터에 대해서만 할 수 있다. 하지만 벡터와 스칼라의 경우에는 관례적으로 다음처럼 1-벡터를 사용하여 스칼라를 벡터로 변환하는 연산을 허용한다. 이를 **브로드캐스팅**이라고 한다.

$$
\begin{bmatrix}
10 \\
11 \\
12
\end{bmatrix}
- 10 = \begin{bmatrix}
10 \\
11 \\
12
\end{bmatrix}
- 10 \cdot 1 = \begin{bmatrix}
10 \\
11 \\
12
\end{bmatrix} - \begin{bmatrix}
10 \\
10 \\
10
\end{bmatrix}
$$

데이터 분석에서는 원래의 데이터 벡터 $x$가 아니라 그 데이터 벡터의 각 원소의 평균값을 뺀 **평균 제거 벡터** 또는 **0-평균 벡터**를 사용하는 경우가 많다.


### 선형 조합

벡터/행렬에 다음처럼 스칼라 값을 곱한 후 더하거나 뺀 것을 벡터/행렬의 **선형조합**(linear combination)이라고 한다. 벡터나 행렬을 선형조합해도 크기는 변하지 않는다.

### 벡터와 벡터의 곱셈

벡터를 곱셈하는 방법은 여러가지가 있지만 여기서는 **내적**(linear product)에 대해서만 다룬다. 벡터 $x$와 벡터 $y$의 내적은 아래와 같이 표기한다.

$$ x^Ty $$

내적은 다음처럼 점(dot)으로 표기하는 경우도 있어서 **닷 프로덕트**(dot product)라고도 부르고 $< x, y >$ 기호로 나타낼 수도 있다.

두 벡터를 내적하기 위한 조건은 아래와 같다.

- 우선 두 벡터의 차원(길이)이 같아야 한다.
- 앞의 벡터가 행벡터이고 뒤의 벡터가 열벡터여야 한다.

이때 내적의 결과는 **스칼라 값이 된다.** 아래는 두 벡터 내적의 예시이다.

$$ x = \begin{bmatrix}
1 \\
2 \\
3
\end{bmatrix}, y = \begin{bmatrix}
4 \\
5 \\
6
\end{bmatrix}
$$

$$ x^Ty = \begin{bmatrix}1 & 2 & 3\end{bmatrix} \begin{bmatrix}4 \\ 5 \\ 6\end{bmatrix} =
1 \cdot 4 + 2 \cdot 5 + 3 \cdot 6 = 32
$$

넘파이에서 벡터와 행렬의 내적은 `dot()`이라는 명령 또는 `@`(at이라고 읽는다) 연산자로 계산한다. 2차원 배열로 표시한 벡터를 내적했을 때는 결과값이 스칼라가 아닌 2차원 배열이다.

```python
x = np.array([[1], [2], [3]])
y = np.array([[4], [5], [6]])

np.dot(x.T, y)
```

### 가중합

벡터의 내적은 가중합을 계산할 때 쓰일 수 있다. **가중합**(weighted sum)이란 복수의 데이터를 단순히 합하는 것이 아니라 각각의 수에 어떤 가중치를 곱한 후 이 곱셈 결과들을 다시 합한 것을 말한다. 만약에 데이터 벡터가 $x = \begin{bmatrix}x1, ...., x_N\end{bmatrix}^T$이고 가중치 벡터가  $w = \begin{bmatrix}w1, ...., w_N\end{bmatrix}^T$ 이면 데이터 벡터의 가중합은 아래와 같다.

$$ w_1 x_1 + ... + w_N x_N = \sum_{i=1}^{N} w_i x_i $$

### 유사도

벡터의 내적은 두 벡터간의 유사도를 계산하는데도 이용할 수 있다.

**유사도**는 두 벡터가 닮은 정도를 정량적으로 나타낸 값으로, 두 벡터가 비슷한 경우에는 유사도가 커지고 비슷하지 않은 경우에는 유사도가 작아진다. 내적을 이용하면 **코사인 유사도**라는 유사도를 계산할 수 있다. 추후 선형대수의 기하학적 의미를 공부할 때 코사인 유사도에 대해 살펴볼 것이다. 

### 제곱합

데이터의 분산이나 표준편차 등을 구하는 경우에는 각각의 데이터를 제곱한 뒤 이 값을 모두 더한 **제곱합**을 계산해야 한다. 이 경우에도 벡터의 내적을 사용하여 $x^Tx$로 쓸 수 있다. 

$$
x^Tx = \begin{bmatrix}x_1 & x_2 & ... & x_N\end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \\ ... \\ x_N \end{bmatrix} = \sum_{i=1}^{N}{x_i}^2
$$

### 행렬에서 교환법칙과 분배법칙

행렬의 곱셈에서는 곱하는 행렬의 순서를 바꾸는 교환법칙이 성립하지 않는다. 덧셈에서는 성립한다. 

전치연산도 마찬가지로 덧셈/뺄셈에 대해 분배법칙이 성립한다. 

$$ {(A+B)}^T = A^T + B^T $$

전치연산과 곱셈의 경우에는 분배법칙이 성립하기는 하지만 전치 연산이 분배되면서 곱셈의 순서가 바뀐다.

$$ (AB)^T = B^TA^T $$

$$ (ABC)^T = C^TB^TA^T $$

### 곱셈의 연결

연속된 행렬의 곱셈은 계산 순서를 임의의 순서로 해도 상관 없다.

$$ ABC = (AB)C = A(BC) $$

### 잔차

예측치와 실젯값의 차이를 **오차**(error) 혹은 **잔차**(residual) $e_i$라고 한다. 이러한 잔찻값을 모든 독립변수 벡터에 대해 구하면 잔차벡터 $e$가 된다.

$$ e_i = y_i - \hat{y_i} = y_i - w^T x_i $$

잔차 벡터를 따라서 아래와 같이 쓸 수 있다.

$$ e = y - Xw $$

### 잔차 제곱합

잔차의 크기는 잔차 벡터의 각 원소를 제곱한 후 더한 **잔차 제곱합**을 이용하여 구한다. 이 값은 $e^T e$로 간단하게 쓸 수 있다. 

$$ \sum_{i=1}^{N}{e_i}^2 = \sum_{i=1}^{N}(y_i-w^Tx_i)^2 = (y-Xw)^T(y-Xw) $$

위 식을 풀어쓰면 아래와 같아진다.

$$ (y-Xw)^T(y-Xw) = y^Ty - w^TX^Ty - y^TXw + w^TX^TXw $$

## 이차형식

위 식에서 마지막 항은 $w^TX^TXw$라는 형태다. 이 식에서 $X^TX$는 정방행렬이 되므로 이 정방행렬을 A라고 이름 붙이면 마지막 항은 $w^TAw$가 된다.

벡터의 **이차형식**이란 이처럼 어떤 벡터와 정방행렬이 '행벡터x정방행렬x열벡터' 형식으로 되어있는 것을 말한다.

$$ x^T A x = \sum_{i=1}^{N} \sum_{j=1}^{N} a_{i, j} x_i  x_j$$

---

## 2.3 행렬의 성질

행렬은 여러 숫자로 이루어져 있으므로 실수처럼 부호나 크기를 정의하기 어렵다. 하지만 부호/크기와 유사한 개념은 정의할 수 있다.

### 정부호와 준정부호

영벡터가 아닌 모든 벡터 $x$에 대해 다음 부등식이 성립하면 행렬 $A$가 **양의 정부호**(positive definite)라고 한다.

$$ x^T A x > 0 $$

만약 이 식이 등호를 포함한다면 **양의 준정부호**(positive semi-definite)라고 햔다.

$$ x^T A x \ge 0 $$

위 방법에 따르면 모든 행렬에 대해 양의 정부호와 준정부호를 정의할 수 있지만 보통 *대칭 행렬에 대해서만 정의한다.* 

행럴의 부호와 마찬가지로 행렬 크기를 정의하는 일도 어렵다. 하지만 **놈**(norm), **대각합**(trace), **행렬식**(determinant) 연산은 행렬을 입력받아 유사한 개념의 숫자를 계산한다.


### 행렬 놈

행렬 놈은 아래와 같이 정의한다.

$$ {\parallel A \parallel}_p = {(\sum_{i=1}^{N} \sum_{j=1}^{N} {|a_{i, j}|}^p)}^{\frac{1}{p}} $$

행렬 놈에도 여러 정의가 있는데 여기에서는 **요소별 행렬 놈**(entrywise matrix norm)을 따른다.

$p$는 보통 1, 2 또는 무한대($\infty$)가 사용되는데 이중 $p=2$인 경우가 가장 많이 쓰이므로 $p$ 값 표시가 없는 경우는 $p=2$인 놈이라고 생각하면 된다. $p=2$인 놈을 **프로베니우스 놈**(Frobenius norm)이라고 한다. 

놈의 정의에서 **놈은 항상 0보다 같거나 크다**.

놈은 모든 크기의 행렬에 대해서 정의할 수 있으므로 벡터에 대해서도 정의할 수 있다. 벡터의 놈에서 중요한 성질은 **벡터의 놈이 제곱이 벡터의 제곱합과 같다**는 것이다.

$$ {\parallel x \parallel}_2 = \sum_{i=1}^{N} x_i^2 = x^Tx $$

놈은 0 또는 양수이므로 놈의 제곱이 가장 작을 때 놈도 가장 작아진다. 따라서 **놈을 최소화 하는 것은 벡터의 제곱합을 최소화하는 것과 같다.**